---
title: RL_in_LLM
date: 2025-03-15 13:42:55
tags: RL
categories: 强化学习
---

# 早期的强化学习与大语言模型

这里不涉及到 DeepSeek 提出的 GRPO 方法，只是简单阐述 PPO 与 DPO 方法。

[知乎：强化学习快速入门](https://zhuanlan.zhihu.com/p/27432619345)

---

### **RLHF、PPO与DPO的核心概念及关系解析**

#### **一、核心定义**
1. **RLHF（Reinforcement Learning from Human Feedback，基于人类反馈的强化学习）**  
   • **定义**：RLHF是一种通过人类偏好数据优化大语言模型的框架，旨在使模型输出更符合人类主观判断。其流程分为三个阶段：  
     ◦ **监督微调（SFT）**：在标注数据上训练模型，建立基础能力。  
     ◦ **奖励模型训练（Reward Model, RM）**：利用人类对答案的排序数据训练模型，使其能对输出质量打分。  
     ◦ **策略优化（PPO）**：通过强化学习算法（如PPO）结合奖励模型优化模型参数。  
   • **特点**：依赖人类反馈或奖励模型提供奖励信号，适用于需要主观评价的任务（如对话生成）。

2. **PPO（Proximal Policy Optimization，近端策略优化）**  
   • **定义**：一种强化学习算法，通过限制策略更新的幅度（如裁剪重要性采样比率）来提升训练稳定性。其核心公式为：  
    
    $$ L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min\left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)} A_t, \text{clip}\left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon \right) A_t \right) \right]$$
    
     其中，\( \epsilon \)为裁剪阈值，\( A_t \)为优势函数。  
   • **特点**：解决了传统策略梯度方法的方差大、收敛难问题，广泛应用于RLHF中的策略优化阶段。

3. **DPO（Direct Preference Optimization，直接偏好优化）**  
   • **定义**：一种绕过强化学习的监督学习方法，直接利用人类偏好数据优化模型。其目标函数为：  
     $$L_{\text{DPO}} = -\mathbb{E}_{(x,y_w,y_l)} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]$$
     其中，\( y_w \)为偏好答案，\( y_l \)为劣质答案。  
   • **特点**：无需奖励模型或PPO，训练更高效稳定，但依赖高质量偏好数据。

---

#### **二、联系与区别**
##### **1. 技术层级关系**
• **RLHF是框架，PPO是核心算法**  
  RLHF通过PPO实现策略优化（即，RLHF = SFT + PPO + 人类反馈），而PPO也可独立用于其他强化学习任务。  
• **DPO是替代方案**  
  DPO绕过RLHF中的PPO阶段，直接通过监督学习优化模型，与RLHF形成并行技术路线。

##### **2. 训练流程对比**
| **方法** | **数据依赖**       | **优化机制**           | **适用场景**               |
|----------|--------------------|------------------------|--------------------------|
| **RLHF** | 人类排序数据、奖励模型 | PPO强化学习            | 需动态交互、高主观评价任务（如创意写作） |
| **PPO**  | 环境交互或奖励信号   | 裁剪目标函数+KL散度惩罚 | 通用强化学习场景（如游戏AI、机器人控制） |
| **DPO**  | 人类偏好数据         | 监督学习直接优化        | 偏好数据充足且质量高的场景（如客服回答筛选） |

##### **3. 核心差异点**
• **奖励信号来源**  
  • RLHF：奖励来自人类反馈或奖励模型。  
  • PPO：奖励由环境或任务定义（如游戏得分）。  
  • DPO：直接利用人类标注的偏好对（\( y_w \) vs \( y_l \)）。  
• **计算复杂度**  
  • RLHF+PPO需多阶段训练（SFT→RM→PPO），计算成本高；DPO单阶段监督学习，效率更高。  
• **稳定性**  
  • PPO通过裁剪和KL散度惩罚提升稳定性；DPO避免强化学习的探索-利用困境，训练更可控。

---

#### **三、典型应用场景**
1. **RLHF+PPO**  
   • **案例**：OpenAI的InstructGPT，通过人类标注员对输出排序，训练奖励模型并利用PPO优化生成策略，使回答更符合人类偏好。  
   • **优势**：适合需动态调整策略的场景（如对话系统的上下文连贯性优化）。  

2. **DPO**  
   • **案例**：早期的DeepSeek-V1模型，通过构造高质量偏好数据集（如“正确回答”vs“错误回答”），直接优化模型生成倾向。  
   • **优势**：数据充足时，可快速对齐模型输出与人类需求，减少训练耗时。  

---

#### **四、总结**
• **RLHF与PPO**：RLHF依赖PPO作为策略优化的核心工具，两者共同解决复杂偏好对齐问题，但计算成本较高。  
• **PPO与DPO**：PPO是通用强化学习算法，适用于动态环境；DPO是高效监督方法，依赖静态偏好数据，两者互为补充。  
• **选择建议**：  
  • 若需结合动态交互与人类主观评价，选RLHF+PPO；  
  • 若偏好数据充足且任务目标明确，选DPO。  