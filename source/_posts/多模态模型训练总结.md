---
title: 多模态模型训练总结
date: 2025-04-14 16:54:18
tags: [LLM]
categories: [LLM, 多模态]
banner_img: /img/ViT.png
---


# 基于纯语言模型训练的多模态模型

{% note primary %}
- **based model**: Qwen2.5-1.5B-Instruct（实在是没资源, 4卡 V100 32G）
- **参考文献**:
  - [ViT 原论文:An image is worth 16x16 words: Transformers for image recognition at scale](https://arxiv.org/pdf/2010.11929) （穷人玩不了的玩意）
  - [LLaVA](https://arxiv.org/abs/2310.03744)

{% endnote %}


# 1.核心目标与挑战

- **目标**: 让原本只能理解文本的 Qwen2.5-1.5B 模型，能够同时理解图像和文本输入，并根据这些联合信息生成连贯、相关的文本输出（例如，回答关于图像的问题、描述图像内容、根据图文指令执行任务等）。
- **核心挑战**: 如何将像素形式的视觉信息有效地“翻译”成 Qwen LLM 能够理解的“语言”——也就是向量嵌入（Embeddings）。LLM 的世界是由离散的词元（Tokens）及其对应的嵌入向量构成的，图像信息必须被转换成类似的格式才能被处理。

<br>
<br>

# 2.关键组件的设计思路与细节

## a.视觉编码器（Vision Encoder - VE）

{% note info %}
**目的:** 从输入的图像中提取高层次、有意义的视觉特征。它扮演着模型的“眼睛”。
{% endnote %}

{% note warning %}
**为什么不从头训练?** 

训练一个强大的视觉编码器需要海量的图像数据 （如 ImageNet, LAION 等）和巨大的计算资源。利用社区已经预训练好的模型（如 ViT, ResNet, ConvNeXt, CLIP Vision Encoder）是最实际、最高效的方法。这些模型已经学会了识别图像中的物体、场景、纹理等基础视觉概念。
{% endnote %}

**选择考量:**
- **ViT（Vision Transformer）**: 目前在很多视觉和多模态任务上表现优异，能捕捉图像的全局依赖关系。不同大小的 ViT（Base, Large, Huge）提供了性能和效率的权衡。
- **CLIP Vision Encoder**: 由于 CLIP 本身就是通过对比学习图像和文本训练的，它的视觉特征可能已经与语言概念有一定程度的对齐，这可能对后续的视觉-语言融合有帮助。
- **卷积网络（如 ResNet, ConvNeXt）**: 仍然是强大的选择，尤其是在某些特定任务或资源受限场景下。

<br>

**输出**: 
通常，我们会使用 VE 输出的两种特征之一：
- **【cls】 Token Embedding**: 类似于 BERT，很多 VE（尤其是 ViT）在输入序列前添加一个特殊的 【cls】 标记，其最终输出的 embedding 被认为聚合了整个图像的全局信息。这是一个固定维度的向量。
- **Patch Embeddings**: ViT 将图像分割成小块（Patches），每个 Patch 都会生成一个 embedding。这些 Patch embeddings 包含了更丰富的空间局部信息。我们可以使用所有 Patch embeddings，或者对它们进行池化（如平均池化）。

{% note danger %}
**是否冻结**: 在多模态训练的初始阶段（对齐阶段），==通常冻结视觉编码器的权重==。目的是保留其强大的、预训练好的视觉特征提取能力，避免在有限的多模态数据上被破坏。在后续的端到端微调阶段，有时也会考虑解冻部分层进行微调，但这需要谨慎，并可能需要更多数据。
{% endnote %}


## b.连接模块/投影层 （Connector/Projector）

{% note info %}
**目的**: 这是最关键的“桥梁”组件。它的作用是将视觉编码器输出的特征（其维度和语义空间与 LLM 不同）映射（投影）到 Qwen LLM 的词嵌入空间。它需要学习如何将“视觉概念”翻译成 LLM 能理解的“词元等价物”的嵌入表示。
{% endnote %}

**设计选择**:
- **简单线性层**: 最简单直接的方式。将 VE 输出的特征向量（例如 【CLS】 embedding）通过一个 nn.Linear 层，直接映射到 LLM 的隐藏层维度 （llm_hidden_size）。

```python
projector = nn.Linear(vision_hidden_size, llm_hidden_size)
```

- **MLP（多层感知机）**: 在线性层基础上增加非线性（如 GELU, ReLU 激活函数）和可能存在的层归一化（LayerNorm）。这提供了更强的表示能力，允许更复杂的特征转换。（==ViT 就用的MLP==）
```python

projector = nn.Sequential(
    nn.Linear(vision_hidden_size, llm_hidden_size),
    nn.GELU(),
    nn.Linear(llm_hidden_size, llm_hidden_size)
)
```

- **更复杂的结构 （如带有 Cross-Attention）**: 像 Flamingo 模型那样，引入交叉注意力机制，让文本 token 可以“查询”视觉特征。这通常涉及到更深层次的模型架构修改，对于仅适配现有 LLM 的情况（如 LLaVA 方式）不太常用，线性层或 MLP 是主流。

>**输出维度**: 目标维度必须与 Qwen LLM 的词嵌入维度（llm.config.hidden_size）完全一致。

**输出数量 （代表图像的 Token 数）**:
- **单个向量**: 如果只使用 【CLS】 token 并投影一次，那么整个图像就被表示为一个向量。这很简洁，但可能丢失细节。
- **多个向量**:
  - 可以将 【CLS】 token 投影到一个更高维的空间，然后 reshape 成 （num_image_tokens, llm_hidden_size）。
  - ==更常见的是== 利用 VE 输出的 Patch Embeddings。例如，LLaVA 使用 ViT 输出的所有 Patch Embeddings，通过一个 MLP 投影层将每个 Patch Embedding 映射到 LLM 的 hidden_size，从而得到一组（例如 LLaVA 中是 256 个）代表图像的向量序列。这能保留更多空间信息。

{% note success %}
**训练重点**: 在训练的第一阶段（对齐阶段），连接模块是主要（甚至唯一）被训练的部分。它需要学会如何进行有效的“翻译”。
{% endnote %}

## c.语言模型 （LLM - Qwen2.5-1.5B）
{% note info %}
**角色**: 作为模型的“大脑”，负责理解融合了视觉信息的输入序列，并进行推理、生成文本。
{% endnote %}

**输入**: 它不再仅仅接收文本 token 的嵌入，而是接收一个混合序列，其中包含了代表图像的特殊嵌入。

**训练策略**:
- **初始阶段 （对齐阶段）**: 通常完全冻结 LLM 的权重。目的是利用 Qwen 强大的语言先验知识，让连接模块学习如何生成 Qwen 能“读懂”的视觉嵌入。如果此时训练 LLM，可能会导致灾难性遗忘，破坏其语言能力。
- **后续阶段 （指令微调阶段）**: 为了让模型能够执行具体的、复杂的图文任务（而不只是简单描述），需要解冻 LLM 的部分或全部权重进行微调。这时，模型需要学习如何在视觉背景下进行推理和遵循指令。为了效率和防止遗忘，通常采用参数高效微调 （Parameter-Efficient Fine-Tuning, PEFT） 技术，如 LoRA （Low-Rank Adaptation）。LoRA 只训练少量额外的低秩矩阵，而不是整个 LLM 的权重，大大降低了计算和存储成本。

{% note success %}
这一步跟 **"冷启动"** 有点像，最少用一个epoch先把模型对齐（其实就是获得能力），然后再进行SFT

可以参考同站博客[kimi1.5 technical report](https://yyhchen.github.io/2025/03/22/kimi1-5-technical-report/)， 里面简要介绍了 "冷启动"
{% endnote %}

<br>
<br>

# 3.输入表示与序列构建 （关键细节）

{% note info %}
**如何将图像和文本组合成一个 LLM 能处理的序列？**
{% endnote %}

**LLaVA 的方法很有代表性**：

- 1.**特殊图像标记**: 在 LLM 的词汇表中添加一个或多个特殊的 token，例如 【image】。这个 token 本身没有意义，只是一个占位符。
- 2.**图像嵌入序列**: 图像经过 VE 和 Projector 处理后，得到一个向量序列 【img_emb_1, img_emb_2, ..., img_emb_N】，其中 N 是 num_image_tokens。
- 3.**文本处理**: 将输入的文本（例如，用户的提问 “【image】\n 这张图片里有什么？”）使用 Qwen 的 Tokenizer 进行分词，得到 token ID 序列。
- 4.**序列拼接**:
    - 找到文本 token ID 序列中代表 【image】 的那个 ID。
    - 获取文本序列中 【image】 之前和之后部分的词嵌入。
    - 将 【image】 token 对应的（单个）嵌入替换为步骤 2 中得到的整个图像嵌入序列 【img_emb_1, ..., img_emb_N】。
    - 最终形成的输入嵌入序列看起来像：【Emb（CLS）, Emb（这）, Emb（张）, ..., Emb（图）, Emb（片）, img_emb_1, ..., img_emb_N, Emb（换行符）, Emb（里）, Emb（有）, Emb（什）, Emb（么）, Emb（？）】。
- 5.**注意力掩码 （Attention Mask）**: 需要构建一个对应的 Attention Mask，确保 LLM 在处理这个混合序列时，每个位置都能注意到所有其他相关的真实 token/embedding（包括图像嵌入和文本嵌入）。掩码的长度等于混合序列的总长度。
- 6.**位置编码 （Positional Encoding）**: LLM 依赖位置编码来理解 token 的顺序。在插入了图像嵌入序列后，需要确保位置编码能够正确地应用于这个加长的序列。不同的 LLM 实现可能有不同的处理方式。
- 7.**标签 （Labels for Training）**: 在训练 Causal LM 时，目标是预测下一个 token。对于这个混合序列：
    - 文本部分的 token，其 label 是下一个 token 的 ID。
    - 图像嵌入部分 【img_emb_1, ..., img_emb_N】对应的 label 通常设置为 -100。这是一个特殊的 Pytorch 值，表示在计算损失时忽略这些位置。因为模型不需要（也不能）预测出“图像嵌入”本身，而是要利用这些图像信息来预测后面的文本 token。

<br>
<br>

# 4.训练策略分阶段详解

## a.阶段一：视觉-语言预训练/对齐（Vision-Language Pre-training/Alignment）

{% note info %}
**目标**: 让 LLM 初步理解图像内容。主要是训练 Projector。
{% endnote %}

- **数据**: 大规模的图像-文本对数据集（例如，COCO Captions, Conceptual Captions （CC）, LAION 的子集）。数据形式简单：一张图片 + 一段描述文字。
- **模型状态**: 冻结 VE，冻结 LLM，只训练 Projector。
- **输入输出**: 输入是图像 + 对应的标题/描述。模型的目标是根据图像嵌入（由 Projector 生成）来预测出这段标题/描述文字。
- **损失函数**: 标准的交叉熵损失，但只计算在文本部分的 token 上。

## b.阶段二：多模态指令微调 （Multimodal Instruction Tuning / Fine-tuning）

{% note info %}
**目标**: 让模型学会遵循复杂的、混合了图像和文本的指令，进行 VQA（视觉问答）、复杂推理、对话等。
{% endnote %}

- **数据**: 格式多样、高质量的多模态指令数据集。例如：
  - **VQA 数据**: 【image】\nQuestion: What color is the sky?\nAnswer: Blue.
  - **描述任务**: 【image】\nDescribe the scene in detail.\nAnswer: A dog is playing fetch in a park...
  - **复杂推理**: 【image】\nBased on the image, is the person likely happy or sad? Explain why.\nAnswer: Happy, because they are smiling.
- 这些数据集通常需要人工构建或者通过强大的模型（如 GPT-4V）生成。LLaVA 项目发布了这类数据集。
- **模型状态**: 冻结 VE，训练 Projector，训练 LLM （通常使用 LoRA）。
- **输入输出**: 输入是指令格式的图文混合内容。模型的目标是生成指令中要求的回答部分。
- **损失函数**: 交叉熵损失，但只计算在答案部分的 token 上（问题、指令、图像部分的 token label 设为 -100）。


<br>
<br>

# 5.总结关键设计考量与权衡

- **VE 的选择**: 平衡性能、速度和对特定任务的适应性。
- **Projector 的复杂度**: 简单（线性层） vs 复杂（MLP），参数量和表达能力的权衡。
- **图像表示的粒度 （num_image_tokens）**: 单向量（紧凑但可能信息不足） vs 多向量（更丰富但序列更长，计算量更大）。
- **训练数据**: 极其重要。数据的质量、数量和多样性直接决定了模型的最终能力。对齐阶段需要大量图文对，微调阶段需要高质量指令数据。
- **训练策略**: 先对齐再微调是常见且有效的方法。冻结哪些部分、何时解冻、是否使用 PEFT 技术都需要根据资源和目标进行选择。

<br>
<br>

# 6.实现中的问题

- **数据处理**: 需要编写复杂的 Data Collator 来处理批次中尺寸不一的图像和长度不同的文本，并将它们正确地整合成模型所需的混合输入格式。
- **内存管理**: 多模态模型，尤其是包含大型 LLM 和 VE 的模型，对 GPU VRAM 的需求非常高。需要使用梯度累积、混合精度训练 (fp16/bf16)、模型并行/数据并行 （DeepSpeed, FSDP）、梯度检查点等技术。
- **调试**: 定位模型不工作的原因可能很困难，问题可能出在 VE、Projector、LLM、数据处理、输入构建或训练逻辑的任何一个环节。


>PS: 本文只是好奇纯语言模型训练多模态问题，可能有些并不是很严谨，欢迎指正。